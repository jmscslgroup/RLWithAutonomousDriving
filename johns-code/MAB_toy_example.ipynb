{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"policy gradient MAB","version":"0.3.2","provenance":[{"file_id":"1xPvd1v7t9FaG1KpLl8rxnH-t-jqrWPEv","timestamp":1561143571780}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"OzDIfEWKMXc-","colab_type":"code","colab":{}},"source":["import numpy as np\n","import sys\n","\n","\n","def MAB(n, trials, bias):\n","  # Set printing options\n","  np.set_printoptions(3,suppress=True)\n","  \n","  # Randomize\n","  np.random.seed(np.random.randint(67))\n","\n","  ground_truth_prob = np.random.rand(n)\n","  number_of_levers_count = np.zeros(n)\n","  agents_prob  = bias\n","  reward_count = np.zeros(n)\n","\n","  num_episode = trials\n","  e = 0.99\n","  learning_rate = 0.5   \n","\n","  for _ in range(num_episode):\n","\n","      # Either choose a greedy action or explore\n","      if e > np.random.uniform():\n","          which_lever_to_pull = np.random.randint(0,n)\n","      else:\n","          which_lever_to_pull = np.argmax(agents_prob)\n","\n","      # now pull the lever \n","      if ground_truth_prob[which_lever_to_pull] > np.random.uniform():\n","          reward = 1\n","      else:\n","          reward = 0\n","\n","      # now update the lever count and expected value\n","      number_of_levers_count[which_lever_to_pull] += 1\n","      # print(which_lever_to_pull)\n","      loss =  - np.log( agents_prob[which_lever_to_pull] + 1e-3 ) * reward \n","      d_loss = (-1/(agents_prob[which_lever_to_pull]+1e-3)) * reward\n","      agents_prob[which_lever_to_pull] = agents_prob[which_lever_to_pull] - learning_rate * d_loss # this is learned by gradient desent\n","      reward_count[which_lever_to_pull] = reward_count[which_lever_to_pull] + reward\n","      e = e * 0.9999\n","  \n","  d_loss = (1/(agents_prob)) * reward\n","  #print(d_loss)\n","  return loss, agents_prob, d_loss, ground_truth_prob\n","\n","  #print('\\n-------------------------')\n","  #print(\"Number of Levers Count Each: \", number_of_levers_count)\n","  #print(\"Sum of lever must match with # episode : \",number_of_levers_count.sum(), ' : ', num_episode)#\n","\n","  #print('\\n-------------------------')\n","  #print(\"Reward Over Time Tracking : \", reward_count) \n","  #print(\"Agent's weight : \", agents_prob)\n","  #print(\"Agent's Probability Guess : \", reward_count/number_of_levers_count )\n","  #print(\"Agents Guess Best Lever : \", np.argmax(reward_count/number_of_levers_count))\n","\n","  #print('\\n-------------------------')\n","  #print(\"Probability of each lever Ground Truth Full list : \", ground_truth_prob)\n","  #print(\"Ground Truth Best Lever : \", np.argmax(ground_truth_prob))\n","  \n","def normalize(array):\n","  sum = 0\n","  for i in range(len(array)):\n","    sum += array[i]\n","  for i in range(len(array)):\n","    array[i] = array[i] / sum\n","  return array\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-KIluXsdohF8","colab_type":"code","outputId":"8604115f-9dcd-4b21-c5aa-7d3a67ac9f61","executionInfo":{"status":"ok","timestamp":1561345155804,"user_tz":420,"elapsed":74709,"user":{"displayName":"John Vuong Nguyen","photoUrl":"","userId":"08823559116611168029"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["np.random.seed(67)\n","\n","n = 2\n","\n","# randomize agents_prob\n","init_bias = np.random.rand(n)\n","\n","print('pre normalize:', init_bias)\n","\n","init_bias = normalize(init_bias)\n","\n","print('post normalize:', init_bias)\n","\n","bias = init_bias/2\n","bias = bias * 2\n","new_bias = init_bias\n","prob_list = np.zeros((4, n))\n","losses = 0\n","d_losses = np.zeros(n)\n","meta_l_rate = 0.01\n","\n","#print('initial bias is: ', init_bias)\n","print('\\n\\n')\n","\n","#Train MAML on MAB n = 9\n","\n","\n","for k in range(10000):\n","  losses, agent_prob, d_losses, true_prob = MAB(n, 1000, bias) # So I can test in the order 9, 7, 5, 3\n","\n","  #print(losses)\n","  #print('True Bandit Probabilities:', true_prob)\n","  #print(d_losses)\n","\n","  #print('\\n')\n","\n","  new_bias = new_bias - (meta_l_rate * d_losses)\n","  bias = new_bias/2\n","  bias = bias * 2\n","\n","print('initial bias:', init_bias)\n","\n","print('final bias:', new_bias)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["pre normalize: [0.546 0.859]\n","post normalize: [0.389 0.611]\n","\n","\n","\n","initial bias: [0.389 0.611]\n","final bias: [-0.001 -0.001]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UeCjLtmfa0Yn","colab_type":"code","outputId":"00295597-bbee-4106-b3bc-72bbfaf6e94a","executionInfo":{"status":"ok","timestamp":1561156856222,"user_tz":420,"elapsed":389,"user":{"displayName":"John Vuong Nguyen","photoUrl":"","userId":"08823559116611168029"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print(d_losses[1])\n","print(init_prob[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-0.10658333963114107\n","9.38232939088563\n"],"name":"stdout"}]}]}